
---
title: "Metropolis-Hastings Algorithm Demo"
author: "Ethan Smith"
output: 
  html_document: 
    theme: cerulean
    code_folding: hide
editor_options: 
  chunk_output_type: inline
---


## Introduction 

The **Metropolis_Hastings algorithm** is a Markov Chain Monte Carlo method for generating samples from a target distribution when direct sampling is difficult. It constructs a Markov chain that has the target distribution as its stationary distribution.


For this project, we implement a 2-dimensional version of the algorithm, targeting a bivariate distribution. 

\[
f(a, b) \propto \exp(-2a^2 - 2ab - b^2)
\]

Since this isn't a standard distribution we use Markov Chain Monte Carlo to approximate samples from it. 


---

## How the Metropolis-Hastings Algorithm Works

The algorithm operates as follows:

1. **Initialization**: Start at a point in the domain.
2. **Proposal Step**: Generate a candidate point from a proposal distribution.
3. **Acceptance Step**: Compute the acceptance ratio:

\[
\alpha = \min\left(1, \frac{f(\text{candidate}) \cdot q(\text{current}|\text{candidate})}{f(\text{current}) \cdot q(\text{candidate}|\text{current})}\right)
\]

Where:
- \( f \) is the unnormalized target density
- \( q \) is the proposal density

4. **Decision Step**: Accept the candidate with probability \( \alpha \). Otherwise, stay at the current point.
5. **Repeat**: Run many iterations to approximate the target distribution.

---

## R Implementation

We use several R programming features in this project:

- Lists and data frames
- Custom functions
- Loops
- Different data types: numeric, character, logical, list, dataframe
- Output to screen
- Reading/writing CSV files


```{r setup, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse)
set.seed(42)

# Target density (unnormalized)
target_density <- function(x, y) {
  exp(-2 * x^2 - 2 * x * y - y^2)
}

# Proposal density: symmetric Gaussian
proposal_density <- function(x1, y1, x0, y0, var) {
  dnorm(x1, x0, sqrt(var)) * dnorm(y1, y0, sqrt(var))
}

# Acceptance logic
accept_candidate <- function(x0, y0, x1, y1, var) {
  alpha <- min(1,
    target_density(x1, y1) / target_density(x0, y0) *
      proposal_density(x0, y0, x1, y1, var) /
      proposal_density(x1, y1, x0, y0, var)
  )
  u <- runif(1)
  list(accept = u < alpha, u = u, alpha = alpha)
}

# Settings
iterations <- 50000
burn_in <- 1000
proposal_var <- 1.5^2

# Start values
current_x <- 1
current_y <- -1

# Store samples in a list before converting to a dataframe
samples <- vector("list", iterations)

for (i in 1:iterations) {
  # Propose
  x_prop <- rnorm(1, current_x, sqrt(proposal_var))
  y_prop <- rnorm(1, current_y, sqrt(proposal_var))
  
  # Accept/reject
  result <- accept_candidate(current_x, current_y, x_prop, y_prop, proposal_var)
  accepted <- result$accept
  
  if (accepted) {
    current_x <- x_prop
    current_y <- y_prop
  }
  
  # Save results
  samples[[i]] <- list(
    iteration = i,
    x = current_x,
    y = current_y,
    accepted = accepted,
    u = result$u,
    alpha = result$alpha
  )
  
  # Switch-style message every 10000
  if (i %% 10000 == 0) {
    msg <- switch(as.character(i),
      "10000" = "25% done...",
      "20000" = "Halfway there!",
      "30000" = "75% complete...",
      "40000" = "Almost done!",
      "50000" = "Finished!"
    )
    print(msg)
  }
}
```



We now process the samples and compute key statistics.

This shows the proportion of accepted proposals, which helps assess how well the algorithm is exploring the space. Acceptance rates between 20â€“50% are often ideal.

These give us empirical probabilities for events under the target distribution, estimated using our Markov chain samples.

```{r}

# Convert to dataframe
mh_df <- bind_rows(samples)

# Save to CSV
write.csv(mh_df, "mh_simulated_output.csv", row.names = FALSE)

# Read back in (to fulfill stretch challenge)
mh_df <- read.csv("mh_simulated_output.csv")


# Remove burn-in
post_burn <- mh_df %>% filter(iteration > burn_in)

# Acceptance rate
post_burn %>%
  count(accepted) %>%
  mutate(rate = n / sum(n))

# Estimate P(x < -1)
mean(post_burn$x < -1)

# Estimate P(y > 2)
mean(post_burn$y > 2)

# Estimate P(x * y < 0)
mean(post_burn$x * post_burn$y < 0)


```

Finally we create a 2D density plot showing the density of x and y after burn in.This plot shows where the samples are most concentrated, which helps visualize the shape of the distribution we were targeting.

```{r}
# 2D Density Plot
ggplot(post_burn, aes(x = x, y = y)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", color = "white") +
  labs(title = "Joint Density Estimate", x = "x", y = "y") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))


```

## Conclusion 

In this notebook, we successfully implemented the Metropolis-Hastings algorithm to simulate samples from a complex bivariate distribution. The project demonstrates:

Core logic of MCMC

Effective use of loops, lists, and data frames in R

Saving and reloading data from CSV

Meaningful statistical summaries

Visualization of the resulting distribution

This technique is broadly useful in Bayesian statistics and any scenario where direct sampling is difficult.
